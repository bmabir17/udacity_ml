{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting bangla dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BanglaLekha-Isolated already present - Skipping extraction of ./BanglaLekha-Isolated.zip\n",
      "['./BanglaLekha-Isolated/1', './BanglaLekha-Isolated/10', './BanglaLekha-Isolated/11', './BanglaLekha-Isolated/12', './BanglaLekha-Isolated/13', './BanglaLekha-Isolated/14', './BanglaLekha-Isolated/15', './BanglaLekha-Isolated/16', './BanglaLekha-Isolated/17', './BanglaLekha-Isolated/18', './BanglaLekha-Isolated/19', './BanglaLekha-Isolated/2', './BanglaLekha-Isolated/20', './BanglaLekha-Isolated/21', './BanglaLekha-Isolated/22', './BanglaLekha-Isolated/23', './BanglaLekha-Isolated/24', './BanglaLekha-Isolated/25', './BanglaLekha-Isolated/26', './BanglaLekha-Isolated/27', './BanglaLekha-Isolated/28', './BanglaLekha-Isolated/29', './BanglaLekha-Isolated/3', './BanglaLekha-Isolated/30', './BanglaLekha-Isolated/31', './BanglaLekha-Isolated/32', './BanglaLekha-Isolated/33', './BanglaLekha-Isolated/34', './BanglaLekha-Isolated/35', './BanglaLekha-Isolated/36', './BanglaLekha-Isolated/37', './BanglaLekha-Isolated/38', './BanglaLekha-Isolated/39', './BanglaLekha-Isolated/4', './BanglaLekha-Isolated/40', './BanglaLekha-Isolated/41', './BanglaLekha-Isolated/42', './BanglaLekha-Isolated/43', './BanglaLekha-Isolated/44', './BanglaLekha-Isolated/45', './BanglaLekha-Isolated/46', './BanglaLekha-Isolated/47', './BanglaLekha-Isolated/48', './BanglaLekha-Isolated/49', './BanglaLekha-Isolated/5', './BanglaLekha-Isolated/50', './BanglaLekha-Isolated/51', './BanglaLekha-Isolated/52', './BanglaLekha-Isolated/53', './BanglaLekha-Isolated/54', './BanglaLekha-Isolated/55', './BanglaLekha-Isolated/56', './BanglaLekha-Isolated/57', './BanglaLekha-Isolated/58', './BanglaLekha-Isolated/59', './BanglaLekha-Isolated/6', './BanglaLekha-Isolated/60', './BanglaLekha-Isolated/61', './BanglaLekha-Isolated/62', './BanglaLekha-Isolated/63', './BanglaLekha-Isolated/64', './BanglaLekha-Isolated/65', './BanglaLekha-Isolated/66', './BanglaLekha-Isolated/67', './BanglaLekha-Isolated/68', './BanglaLekha-Isolated/69', './BanglaLekha-Isolated/7', './BanglaLekha-Isolated/70', './BanglaLekha-Isolated/71', './BanglaLekha-Isolated/72', './BanglaLekha-Isolated/73', './BanglaLekha-Isolated/74', './BanglaLekha-Isolated/75', './BanglaLekha-Isolated/76', './BanglaLekha-Isolated/77', './BanglaLekha-Isolated/78', './BanglaLekha-Isolated/79', './BanglaLekha-Isolated/8', './BanglaLekha-Isolated/80', './BanglaLekha-Isolated/81', './BanglaLekha-Isolated/82', './BanglaLekha-Isolated/83', './BanglaLekha-Isolated/84', './BanglaLekha-Isolated/9']\n"
     ]
    }
   ],
   "source": [
    "num_classes=84\n",
    "np.random.seed(133)\n",
    "data_root='.'\n",
    "full_filename=os.path.join(data_root, 'BanglaLekha-Isolated.zip')\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root=os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    root=root #as the classesdata are in images folder\n",
    "    if os.path.isdir(root)and not force:\n",
    "        print('%s already present - Skipping extraction of %s' %(root, filename))\n",
    "    else:\n",
    "        print('extracting data %s.this may take a while .please wait.' %root)\n",
    "        zip=zipfile.ZipFile(filename)\n",
    "        sys.stdout.flush()\n",
    "        zip.extractall(data_root)\n",
    "        zip.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d)for d in sorted(os.listdir(root)) #listing all the classes directory\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) !=num_classes: #checks lengths\n",
    "        raise Exception(\n",
    "            'Expected %d folders , one per class. found %d instead.' % (\n",
    "                num_classes,len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "full_folders=maybe_extract(full_filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images into npArray and save into pickle file with separate class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BanglaLekha-Isolated/1.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/10.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/11.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/12.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/13.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/14.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/15.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/16.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/17.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/18.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/19.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/2.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/20.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/21.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/22.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/23.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/24.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/25.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/26.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/27.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/28.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/29.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/3.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/30.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/31.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/32.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/33.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/34.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/35.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/36.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/37.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/38.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/39.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/4.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/40.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/41.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/42.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/43.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/44.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/45.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/46.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/47.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/48.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/49.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/5.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/50.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/51.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/52.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/53.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/54.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/55.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/56.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/57.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/58.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/59.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/6.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/60.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/61.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/62.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/63.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/64.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/65.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/66.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/67.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/68.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/69.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/7.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/70.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/71.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/72.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/73.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/74.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/75.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/76.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/77.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/78.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/79.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/8.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/80.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/81.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/82.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/83.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/84.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/9.pickle already present - skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size=28 #pixel width and height\n",
    "pixel_depth=255.0 #Nurmber of levels per pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label\"\"\"\n",
    "    image_files=os.listdir(folder)\n",
    "    dataset=np.ndarray(shape=(len(image_files),image_size, image_size),dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images=0\n",
    "    for image in image_files:\n",
    "        image_file=os.path.join(folder,image)\n",
    "        try:\n",
    "            image_data=(ndimage.imread(image_file).astype(float) - pixel_depth /2)/pixel_depth\n",
    "            if image_data.shape !=(image_size,image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape),(image_file))\n",
    "            dataset[num_images,:,:]=image_data\n",
    "            num_images=num_images + 1\n",
    "        except IOError as e:\n",
    "            print('couldnot read :', image_file , ':', e , '-it\\'s ok , skipping.')\n",
    "    dataset=dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('many fewer images then expected: %d < %d' % (num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('mean:', np.mean(dataset))\n",
    "    print('standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names=[]\n",
    "    for folder in data_folders:\n",
    "        set_filename=folder+ '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename)and not force:\n",
    "            print('%s already present - skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('pickling %s.' % set_filename)\n",
    "            dataset=load_letter(folder,min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb')as f:\n",
    "                    pickle.dump(dataset,f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':',e)\n",
    "    return dataset_names\n",
    "full_datasets=maybe_pickle(full_folders, 1900)#Not MNIST Large\n",
    "\n",
    "#print (full_datasets.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # shuffle images from each class to have random validation and training set then merge each classes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  (139000, 28, 28) (139000,)\n",
      "validation : (10000, 28, 28) (10000,)\n",
      "test : (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#print (train_dataset.shape)\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset=np.ndarray((nb_rows, img_size , img_size), dtype=np.float32)\n",
    "        labels=np.ndarray(nb_rows,dtype=np.float32)\n",
    "    else:\n",
    "        dataset,labels=None, None\n",
    "    return dataset,labels\n",
    "def merge_datasets(pickle_files,train_size, valid_size=0):\n",
    "    num_classes= len(pickle_files)\n",
    "    valid_dataset, valid_labels= make_arrays(valid_size, image_size)#for validation \n",
    "    train_dataset, train_labels= make_arrays(train_size, image_size)\n",
    "    vsize_per_class= valid_size // num_classes\n",
    "    tsize_per_class= train_size // num_classes\n",
    "    \n",
    "    start_v,start_t =0 , 0\n",
    "    end_v, end_t =vsize_per_class , tsize_per_class\n",
    "    end_l=vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):#label=value or index of pickle files,example A,B,C,D\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set=pickle.load(f)\n",
    "                #shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter=letter_set[:vsize_per_class, : , :]\n",
    "                    valid_dataset[start_v:end_v, : , :] = valid_letter\n",
    "                    valid_labels[start_v:end_v]= label\n",
    "                    start_v +=vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter= letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, : ,:]=train_letter\n",
    "                train_labels[start_t:end_t]=label\n",
    "                start_t +=tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('unable to process data from', pickle_file, '.', e)\n",
    "            raise\n",
    "    return valid_dataset,valid_labels,train_dataset, train_labels\n",
    "\n",
    "train_size= 139000\n",
    "valid_size=10000\n",
    "test_size=10000\n",
    "\n",
    "valid_dataset, valid_labels,train_dataset,train_labels = merge_datasets(full_datasets, train_size , valid_size)\n",
    "test_dataset, test_labels,train_dataset,train_labels=merge_datasets(full_datasets,train_size,test_size)\n",
    "\n",
    "print('training: ' ,train_dataset.shape , train_labels.shape)\n",
    "print('validation :' , valid_dataset.shape , valid_labels.shape)\n",
    "print('test :', test_dataset.shape , test_labels.shape)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139000\n",
      "[ 69211 117112  77560 ..., 102436  95756 137937]\n",
      "10000\n",
      "[3946  293  591 ..., 3664 1936 9171]\n",
      "10000\n",
      "[4037 2544 9698 ..., 8874 8086 2204]\n"
     ]
    }
   ],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    print (labels.shape[0])\n",
    "    print (permutation)\n",
    "    shuffled_dataset= dataset[permutation, : , :]\n",
    "    shuffled_labels= labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset , train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset , test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset , valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1820c65390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(train_dataset [0])\n",
    "print (train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 690800406\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5        -0.48823529 -0.13529412 ..., -0.37058824 -0.5        -0.49607843]\n",
      "  [-0.5        -0.28431374  0.5        ..., -0.24117647 -0.5        -0.48823529]\n",
      "  [-0.5        -0.19019608  0.47254902 ..., -0.35882354 -0.5        -0.49215686]\n",
      "  ..., \n",
      "  [ 0.18627451  0.48823529 -0.12745099 ...,  0.37058824  0.5        -0.07254902]\n",
      "  [ 0.26078433  0.5         0.5        ...,  0.5         0.5        -0.04117647]\n",
      "  [ 0.00196078  0.05686275  0.0372549  ...,  0.45686275  0.5         0.00980392]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ...,  0.5         0.5         0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.5         0.5         0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.5         0.5         0.5       ]\n",
      "  ..., \n",
      "  [ 0.5         0.5         0.49607843 ...,  0.49607843  0.5         0.49607843]\n",
      "  [-0.0254902   0.3392157   0.48431373 ...,  0.47647059  0.32352942\n",
      "   -0.05294118]\n",
      "  [-0.5        -0.37450981 -0.1509804  ..., -0.1627451  -0.38235295 -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.49607843 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  ..., \n",
      "  [-0.5        -0.5        -0.49215686 ..., -0.12745099 -0.5        -0.48431373]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.43333334 -0.5        -0.49607843]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]]\n",
      "\n",
      " ..., \n",
      " [[-0.5        -0.5        -0.48431373 ..., -0.49215686 -0.49607843 -0.5       ]\n",
      "  [-0.5        -0.48431373 -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  [-0.48431373 -0.5        -0.24117647 ..., -0.01372549 -0.5        -0.48039216]\n",
      "  ..., \n",
      "  [-0.48431373 -0.5        -0.00196078 ...,  0.22941177 -0.47647059\n",
      "   -0.49607843]\n",
      "  [-0.49607843 -0.48823529 -0.5        ..., -0.44901961 -0.5        -0.49215686]\n",
      "  [-0.5        -0.49607843 -0.48823529 ..., -0.5        -0.48823529 -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ..., -0.10392157  0.00196078\n",
      "   -0.3509804 ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.35882354\n",
      "   -0.46470588]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.48431373  0.08431373\n",
      "    0.1627451 ]\n",
      "  ..., \n",
      "  [-0.07254902  0.1         0.18235295 ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.48823529  0.35882354  0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.36274511  0.44901961  0.48431373 ..., -0.5        -0.5        -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ...,  0.0882353  -0.31176472 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.5         0.37058824\n",
      "   -0.38627452]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.31568629  0.5        -0.20588236]\n",
      "  ..., \n",
      "  [-0.5         0.31960785  0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.25294119  0.1627451  ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]]]\n",
      "Time: 1.93s\n",
      "valid -> train overlap: 1081 samples\n",
      "test  -> train overlap: 1278 samples\n",
      "test  -> valid overlap: 185 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "f=open(pickle_file, 'rb')\n",
    "read_pickle = pickle.load( f )\n",
    "train_dataset=read_pickle['train_dataset']\n",
    "print(train_dataset)\n",
    "train_labels=read_pickle['train_labels']\n",
    "valid_dataset=read_pickle['valid_dataset']\n",
    "valid_labels=read_pickle['valid_labels']\n",
    "test_dataset=read_pickle['test_dataset']\n",
    "test_labels=read_pickle['test_labels']\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "train_hashes = [hashlib.sha1(x).digest() for x in train_dataset]\n",
    "valid_hashes = [hashlib.sha1(x).digest() for x in valid_dataset]\n",
    "test_hashes  = [hashlib.sha1(x).digest() for x in test_dataset]\n",
    "\n",
    "valid_in_train = np.in1d(valid_hashes, train_hashes)\n",
    "test_in_train  = np.in1d(test_hashes,  train_hashes)\n",
    "test_in_valid  = np.in1d(test_hashes,  valid_hashes)\n",
    "\n",
    "valid_keep = ~valid_in_train\n",
    "test_keep  = ~(test_in_train | test_in_valid)\n",
    "\n",
    "valid_dataset_clean = valid_dataset[valid_keep]\n",
    "valid_labels_clean  = valid_labels [valid_keep]\n",
    "\n",
    "test_dataset_clean = test_dataset[test_keep]\n",
    "test_labels_clean  = test_labels [test_keep]\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))\n",
    "print(\"valid -> train overlap: %d samples\" % valid_in_train.sum())\n",
    "print(\"test  -> train overlap: %d samples\" % test_in_train.sum())\n",
    "print(\"test  -> valid overlap: %d samples\" % test_in_valid.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9307, 28, 28)\n",
      "7296688\n",
      "(1698, 28, 28)\n",
      "1331232\n",
      "(139000, 28, 28)\n",
      "108976000\n"
     ]
    }
   ],
   "source": [
    "print (test_dataset_clean.shape)\n",
    "print (test_dataset_clean.size)\n",
    "print (valid_dataset_clean.shape)\n",
    "print (valid_dataset_clean.size)\n",
    "print (train_dataset.shape)\n",
    "print (train_dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
