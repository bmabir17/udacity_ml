{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting bangla dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BanglaLekha-Isolated already present - Skipping extraction of ./BanglaLekha-Isolated.zip\n",
      "['./BanglaLekha-Isolated/1', './BanglaLekha-Isolated/10', './BanglaLekha-Isolated/11', './BanglaLekha-Isolated/12', './BanglaLekha-Isolated/13', './BanglaLekha-Isolated/14', './BanglaLekha-Isolated/15', './BanglaLekha-Isolated/16', './BanglaLekha-Isolated/17', './BanglaLekha-Isolated/18', './BanglaLekha-Isolated/19', './BanglaLekha-Isolated/2', './BanglaLekha-Isolated/20', './BanglaLekha-Isolated/21', './BanglaLekha-Isolated/22', './BanglaLekha-Isolated/23', './BanglaLekha-Isolated/24', './BanglaLekha-Isolated/25', './BanglaLekha-Isolated/26', './BanglaLekha-Isolated/27', './BanglaLekha-Isolated/28', './BanglaLekha-Isolated/29', './BanglaLekha-Isolated/3', './BanglaLekha-Isolated/30', './BanglaLekha-Isolated/31', './BanglaLekha-Isolated/32', './BanglaLekha-Isolated/33', './BanglaLekha-Isolated/34', './BanglaLekha-Isolated/35', './BanglaLekha-Isolated/36', './BanglaLekha-Isolated/37', './BanglaLekha-Isolated/38', './BanglaLekha-Isolated/39', './BanglaLekha-Isolated/4', './BanglaLekha-Isolated/40', './BanglaLekha-Isolated/41', './BanglaLekha-Isolated/42', './BanglaLekha-Isolated/43', './BanglaLekha-Isolated/44', './BanglaLekha-Isolated/45', './BanglaLekha-Isolated/46', './BanglaLekha-Isolated/47', './BanglaLekha-Isolated/48', './BanglaLekha-Isolated/49', './BanglaLekha-Isolated/5', './BanglaLekha-Isolated/50', './BanglaLekha-Isolated/51', './BanglaLekha-Isolated/52', './BanglaLekha-Isolated/53', './BanglaLekha-Isolated/54', './BanglaLekha-Isolated/55', './BanglaLekha-Isolated/56', './BanglaLekha-Isolated/57', './BanglaLekha-Isolated/58', './BanglaLekha-Isolated/59', './BanglaLekha-Isolated/6', './BanglaLekha-Isolated/60', './BanglaLekha-Isolated/61', './BanglaLekha-Isolated/62', './BanglaLekha-Isolated/63', './BanglaLekha-Isolated/64', './BanglaLekha-Isolated/65', './BanglaLekha-Isolated/66', './BanglaLekha-Isolated/67', './BanglaLekha-Isolated/68', './BanglaLekha-Isolated/69', './BanglaLekha-Isolated/7', './BanglaLekha-Isolated/70', './BanglaLekha-Isolated/71', './BanglaLekha-Isolated/72', './BanglaLekha-Isolated/73', './BanglaLekha-Isolated/74', './BanglaLekha-Isolated/75', './BanglaLekha-Isolated/76', './BanglaLekha-Isolated/77', './BanglaLekha-Isolated/78', './BanglaLekha-Isolated/79', './BanglaLekha-Isolated/8', './BanglaLekha-Isolated/80', './BanglaLekha-Isolated/81', './BanglaLekha-Isolated/82', './BanglaLekha-Isolated/83', './BanglaLekha-Isolated/84', './BanglaLekha-Isolated/9']\n"
     ]
    }
   ],
   "source": [
    "num_classes=84\n",
    "np.random.seed(133)\n",
    "data_root='.'\n",
    "full_filename=os.path.join(data_root, 'BanglaLekha-Isolated.zip')\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root=os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    root=root #as the classesdata are in images folder\n",
    "    if os.path.isdir(root)and not force:\n",
    "        print('%s already present - Skipping extraction of %s' %(root, filename))\n",
    "    else:\n",
    "        print('extracting data %s.this may take a while .please wait.' %root)\n",
    "        zip=zipfile.ZipFile(filename)\n",
    "        sys.stdout.flush()\n",
    "        zip.extractall(data_root)\n",
    "        zip.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d)for d in sorted(os.listdir(root)) #listing all the classes directory\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) !=num_classes: #checks lengths\n",
    "        raise Exception(\n",
    "            'Expected %d folders , one per class. found %d instead.' % (\n",
    "                num_classes,len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "full_folders=maybe_extract(full_filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images into npArray and save into pickle file with separate class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BanglaLekha-Isolated/1.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/10.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/11.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/12.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/13.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/14.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/15.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/16.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/17.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/18.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/19.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/2.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/20.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/21.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/22.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/23.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/24.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/25.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/26.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/27.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/28.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/29.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/3.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/30.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/31.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/32.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/33.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/34.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/35.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/36.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/37.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/38.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/39.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/4.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/40.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/41.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/42.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/43.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/44.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/45.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/46.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/47.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/48.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/49.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/5.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/50.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/51.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/52.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/53.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/54.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/55.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/56.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/57.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/58.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/59.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/6.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/60.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/61.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/62.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/63.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/64.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/65.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/66.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/67.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/68.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/69.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/7.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/70.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/71.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/72.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/73.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/74.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/75.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/76.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/77.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/78.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/79.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/8.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/80.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/81.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/82.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/83.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/84.pickle already present - skipping pickling.\n",
      "./BanglaLekha-Isolated/9.pickle already present - skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size=28 #pixel width and height\n",
    "pixel_depth=255.0 #Nurmber of levels per pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label\"\"\"\n",
    "    image_files=os.listdir(folder)\n",
    "    dataset=np.ndarray(shape=(len(image_files),image_size, image_size),dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images=0\n",
    "    for image in image_files:\n",
    "        image_file=os.path.join(folder,image)\n",
    "        try:\n",
    "            image_data=(ndimage.imread(image_file).astype(float) - pixel_depth /2)/pixel_depth\n",
    "            if image_data.shape !=(image_size,image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape),(image_file))\n",
    "            dataset[num_images,:,:]=image_data\n",
    "            num_images=num_images + 1\n",
    "        except IOError as e:\n",
    "            print('couldnot read :', image_file , ':', e , '-it\\'s ok , skipping.')\n",
    "    dataset=dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('many fewer images then expected: %d < %d' % (num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('mean:', np.mean(dataset))\n",
    "    print('standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names=[]\n",
    "    for folder in data_folders:\n",
    "        set_filename=folder+ '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename)and not force:\n",
    "            print('%s already present - skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('pickling %s.' % set_filename)\n",
    "            dataset=load_letter(folder,min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb')as f:\n",
    "                    pickle.dump(dataset,f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':',e)\n",
    "    return dataset_names\n",
    "full_datasets=maybe_pickle(full_folders, 1900)#Not MNIST Large\n",
    "\n",
    "#print (full_datasets.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # shuffle images from each class to have random validation and training set then merge each classes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  (139000, 28, 28) (139000,)\n",
      "validation : (10000, 28, 28) (10000,)\n",
      "test : (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#print (train_dataset.shape)\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset=np.ndarray((nb_rows, img_size , img_size), dtype=np.float32)\n",
    "        labels=np.ndarray(nb_rows,dtype=np.float32)\n",
    "    else:\n",
    "        dataset,labels=None, None\n",
    "    return dataset,labels\n",
    "def merge_datasets(pickle_files,train_size, valid_size=0):\n",
    "    num_classes= len(pickle_files)\n",
    "    valid_dataset, valid_labels= make_arrays(valid_size, image_size)#for validation \n",
    "    train_dataset, train_labels= make_arrays(train_size, image_size)\n",
    "    vsize_per_class= valid_size // num_classes\n",
    "    tsize_per_class= train_size // num_classes\n",
    "    \n",
    "    start_v,start_t =0 , 0\n",
    "    end_v, end_t =vsize_per_class , tsize_per_class\n",
    "    end_l=vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):#label=value or index of pickle files,example A,B,C,D\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set=pickle.load(f)\n",
    "                #shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter=letter_set[:vsize_per_class, : , :]\n",
    "                    valid_dataset[start_v:end_v, : , :] = valid_letter\n",
    "                    valid_labels[start_v:end_v]= label\n",
    "                    start_v +=vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter= letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, : ,:]=train_letter\n",
    "                train_labels[start_t:end_t]=label\n",
    "                start_t +=tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('unable to process data from', pickle_file, '.', e)\n",
    "            raise\n",
    "    return valid_dataset,valid_labels,train_dataset, train_labels\n",
    "\n",
    "train_size= 139000\n",
    "valid_size=10000\n",
    "test_size=10000\n",
    "\n",
    "valid_dataset, valid_labels,train_dataset,train_labels = merge_datasets(full_datasets, train_size , valid_size)\n",
    "test_dataset, test_labels,train_dataset,train_labels=merge_datasets(full_datasets,train_size,test_size)\n",
    "\n",
    "print('training: ' ,train_dataset.shape , train_labels.shape)\n",
    "print('validation :' , valid_dataset.shape , valid_labels.shape)\n",
    "print('test :', test_dataset.shape , test_labels.shape)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139000\n",
      "[54740 25921 54452 ..., 70445 63056  7162]\n",
      "10000\n",
      "[5448 6261  849 ..., 8146 6939 4309]\n",
      "10000\n",
      "[4254 4474 9466 ..., 1087 4600 1649]\n"
     ]
    }
   ],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    print (labels.shape[0])\n",
    "    print (permutation)\n",
    "    shuffled_dataset= dataset[permutation, : , :]\n",
    "    shuffled_labels= labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset , train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset , test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset , valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC8pJREFUeJzt3W+oHXedx/H315qmbKzQrG6ItVgLVShlN8IlXbCI0q3W\nUkh9UuwDiVCMD1xZwQdbug+2D4usFR8sQrShUbpVQUvzoGxpw0IRJPS29K9VW0ukyaaJboRWF9O0\nfn1wJ+W2vfeckzMzZ+bm+37B4cyZM+fON3PvJ3POfOfMLzITSfW8a+gCJA3D8EtFGX6pKMMvFWX4\npaIMv1SU4ZeKMvxSUYZfKurdi1zZ+bE5L2DLIlcplfJn/sRreSpmWbZV+CPiOuDbwHnA9zLzjknL\nX8AWropr2qxS0gSH8uDMy879tj8izgP+E/gscAVwc0RcMe/Pk7RYbT7z7wReyMwXM/M14IfArm7K\nktS3NuG/GHhp1eMjzby3iIg9EbEcEcunOdVidZK61PvR/szcm5lLmbm0ic19r07SjNqE/yhwyarH\nH2zmSdoA2oT/UeDyiPhwRJwPfB440E1Zkvo2d6svM1+PiH8GHmSl1bcvM5/trDJJvWrV58/MB4AH\nOqpF0gJ5eq9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0Ut9NLd\nfXrwf5+Y+PxnPrBjQZVIG4N7fqkowy8VZfilogy/VJThl4oy/FJRhl8qaqF9/o/8/f/z4IOT+/Hz\nmtbHn3YewLnK8xu0Hvf8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1RUZOb8L444DLwKvAG8nplLk5Z/\nb2zNq+Kaudens9f2/AbPE9hYDuVBXsmTMcuyXZzk86nM/H0HP0fSAvm2XyqqbfgTeDgiHouIPV0U\nJGkx2r7tvzozj0bE3wEPRcQvM/OR1Qs0/ynsAbiAv2m5OkldabXnz8yjzf0J4D5g5xrL7M3Mpcxc\n2sTmNquT1KG5wx8RWyLiwjPTwKeBZ7oqTFK/2rzt3wbcFxFnfs5/ZeZ/d1KVpN7NHf7MfBH4hw5r\n0QgNeR2Ettdo8ByFyWz1SUUZfqkowy8VZfilogy/VJThl4raUJfutnUzPm3acX1fbr3PNuW58Lfo\nnl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXimp16e6zNe3S3X5Fs3tu0/mMeUj3Sb+zs7l0t3t+qSjD\nLxVl+KWiDL9UlOGXijL8UlGGXypqQ/X5J6ncrx7yO/OVt3tf2vxOdn7mJZaf/LN9fknrM/xSUYZf\nKsrwS0UZfqkowy8VZfiloqZetz8i9gE3ACcy88pm3lbgR8ClwGHgpsz8Q39l1jbk9eenrdtzM7rX\nZrv8Ov9v5mVn2fPfDVz3tnm3Agcz83LgYPNY0gYyNfyZ+Qhw8m2zdwH7m+n9wI0d1yWpZ/N+5t+W\nmcea6ZeBbR3VI2lBWh/wy5UvB6z7BYGI2BMRyxGxfJpTbVcnqSPzhv94RGwHaO5PrLdgZu7NzKXM\nXNrE5jlXJ6lr84b/ALC7md4N3N9NOZIWZWr4I+Je4OfARyPiSETcAtwBXBsRzwP/1DyWtIFM7fNn\n5s3rPLX+F/Pn1Pd3z89VffbL2/7sSb8zxxQYlmf4SUUZfqkowy8VZfilogy/VJThl4qa2urbKGwb\njdOk7W7rdlju+aWiDL9UlOGXijL8UlGGXyrK8EtFGX6pqA3V57dnvLY2Q3SPmedu9Ms9v1SU4ZeK\nMvxSUYZfKsrwS0UZfqkowy8VtaH6/G1s5J7xuXpJ83P137VRuOeXijL8UlGGXyrK8EtFGX6pKMMv\nFWX4paKm9vkjYh9wA3AiM69s5t0OfAn4XbPYbZn5QF9FzqJtz/hcPQ9gzP8u+/jDmmXPfzdw3Rrz\nv5WZO5rboMGXdPamhj8zHwFOLqAWSQvU5jP/VyPiqYjYFxEXdVaRpIWYN/zfAS4DdgDHgG+ut2BE\n7ImI5YhYPs2pOVcnqWtzhT8zj2fmG5n5F+C7wM4Jy+7NzKXMXNrE5nnrlNSxucIfEdtXPfwc8Ew3\n5UhalFlaffcCnwTeFxFHgH8HPhkRO4AEDgNf7rFGST2IzFzYyt4bW/OquGZh6zsbbXrOQ58DsJFr\nn2TM5yiM1aE8yCt5MmZZ1jP8pKIMv1SU4ZeKMvxSUYZfKsrwS0WVuXT3NG2+Ety2JdXnV1vP5XaY\nrcB23PNLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlH2+WfUpmdsP1pj5J5fKsrwS0UZfqkowy8VZfil\nogy/VJThl4qyz78A9vE1Ru75paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmoqX3+iLgE+D6wDUhgb2Z+\nOyK2Aj8CLgUOAzdl5h/6K1V6K8+faGeWPf/rwNcz8wrgH4GvRMQVwK3Awcy8HDjYPJa0QUwNf2Ye\ny8zHm+lXgeeAi4FdwP5msf3AjX0VKal7Z/WZPyIuBT4GHAK2Zeax5qmXWflYIGmDmDn8EfEe4CfA\n1zLzldXPZWaycjxgrdftiYjliFg+zalWxUrqzkzhj4hNrAT/nsz8aTP7eERsb57fDpxY67WZuTcz\nlzJzaRObu6hZUgemhj8iArgLeC4z71z11AFgdzO9G7i/+/Ik9WWWr/R+HPgC8HREnLkG9W3AHcCP\nI+IW4LfATf2UKK3NS6K3MzX8mfkzINZ5+ppuy5G0KJ7hJxVl+KWiDL9UlOGXijL8UlGGXyrKS3dr\nw7KP3457fqkowy8VZfilogy/VJThl4oy/FJRhl8qyj6/Rss+fr/c80tFGX6pKMMvFWX4paIMv1SU\n4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRU8MfEZdExP9ExC8i4tmI+Jdm/u0R\ncTQinmhu1/dfrqSuzHIxj9eBr2fm4xFxIfBYRDzUPPetzPyP/sqT1Jep4c/MY8CxZvrViHgOuLjv\nwiT166w+80fEpcDHgEPNrK9GxFMRsS8iLlrnNXsiYjkilk9zqlWxkrozc/gj4j3AT4CvZeYrwHeA\ny4AdrLwz+OZar8vMvZm5lJlLm9jcQcmSujBT+CNiEyvBvyczfwqQmccz843M/AvwXWBnf2VK6tos\nR/sDuAt4LjPvXDV/+6rFPgc80315kvoyy9H+jwNfAJ6OiCeaebcBN0fEDiCBw8CXe6lQUi9mOdr/\nMyDWeOqB7suRtCie4ScVZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8\nUlGGXyoqMnNxK4v4HfDbVbPeB/x+YQWcnbHWNta6wNrm1WVtH8rM98+y4ELD/46VRyxn5tJgBUww\n1trGWhdY27yGqs23/VJRhl8qaujw7x14/ZOMtbax1gXWNq9Bahv0M7+k4Qy955c0kEHCHxHXRcSv\nIuKFiLh1iBrWExGHI+LpZuTh5YFr2RcRJyLimVXztkbEQxHxfHO/5jBpA9U2ipGbJ4wsPei2G9uI\n1wt/2x8R5wG/Bq4FjgCPAjdn5i8WWsg6IuIwsJSZg/eEI+ITwB+B72fmlc28bwAnM/OO5j/OizLz\nX0dS2+3AH4ceubkZUGb76pGlgRuBLzLgtptQ100MsN2G2PPvBF7IzBcz8zXgh8CuAeoYvcx8BDj5\nttm7gP3N9H5W/ngWbp3aRiEzj2Xm4830q8CZkaUH3XYT6hrEEOG/GHhp1eMjjGvI7wQejojHImLP\n0MWsYVszbDrAy8C2IYtZw9SRmxfpbSNLj2bbzTPiddc84PdOV2fmDuCzwFeat7ejlCuf2cbUrplp\n5OZFWWNk6TcNue3mHfG6a0OE/yhwyarHH2zmjUJmHm3uTwD3Mb7Rh4+fGSS1uT8xcD1vGtPIzWuN\nLM0Itt2YRrweIvyPApdHxIcj4nzg88CBAep4h4jY0hyIISK2AJ9mfKMPHwB2N9O7gfsHrOUtxjJy\n83ojSzPwthvdiNeZufAbcD0rR/x/A/zbEDWsU9dlwJPN7dmhawPuZeVt4GlWjo3cAvwtcBB4HngY\n2Dqi2n4APA08xUrQtg9U29WsvKV/CniiuV0/9LabUNcg280z/KSiPOAnFWX4paIMv1SU4ZeKMvxS\nUYZfKsrwS0UZfqmovwJpMxBM11LcLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9bffa84450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(train_dataset [0])\n",
    "print (train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 499260406\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " ..., \n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ...,  0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ...,  0.5  0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]]\n",
      "Time: 1.41s\n",
      "valid -> train overlap: 8302 samples\n",
      "test  -> train overlap: 75 samples\n",
      "test  -> valid overlap: 633 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "f=open(pickle_file, 'rb')\n",
    "read_pickle = pickle.load( f )\n",
    "train_dataset=read_pickle['train_dataset']\n",
    "print(train_dataset)\n",
    "train_labels=read_pickle['train_labels']\n",
    "valid_dataset=read_pickle['valid_dataset']\n",
    "valid_labels=read_pickle['valid_labels']\n",
    "test_dataset=read_pickle['test_dataset']\n",
    "test_labels=read_pickle['test_labels']\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "train_hashes = [hashlib.sha1(x).digest() for x in train_dataset]\n",
    "valid_hashes = [hashlib.sha1(x).digest() for x in valid_dataset]\n",
    "test_hashes  = [hashlib.sha1(x).digest() for x in test_dataset]\n",
    "\n",
    "valid_in_train = np.in1d(valid_hashes, train_hashes)\n",
    "test_in_train  = np.in1d(test_hashes,  train_hashes)\n",
    "test_in_valid  = np.in1d(test_hashes,  valid_hashes)\n",
    "\n",
    "valid_keep = ~valid_in_train\n",
    "test_keep  = ~(test_in_train | test_in_valid)\n",
    "\n",
    "valid_dataset_clean = valid_dataset[valid_keep]\n",
    "valid_labels_clean  = valid_labels [valid_keep]\n",
    "\n",
    "test_dataset_clean = test_dataset[test_keep]\n",
    "test_labels_clean  = test_labels [test_keep]\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))\n",
    "print(\"valid -> train overlap: %d samples\" % valid_in_train.sum())\n",
    "print(\"test  -> train overlap: %d samples\" % test_in_train.sum())\n",
    "print(\"test  -> valid overlap: %d samples\" % test_in_valid.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9307, 28, 28)\n",
      "7296688\n",
      "(1698, 28, 28)\n",
      "1331232\n",
      "(139000, 28, 28)\n",
      "108976000\n"
     ]
    }
   ],
   "source": [
    "print (test_dataset_clean.shape)\n",
    "print (test_dataset_clean.size)\n",
    "print (valid_dataset_clean.shape)\n",
    "print (valid_dataset_clean.size)\n",
    "print (train_dataset.shape)\n",
    "print (train_dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
