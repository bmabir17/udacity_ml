{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Learning\n",
    "Assignment 2Â¶\n",
    "Previously in 1_notmnist.ipynb, we created a pickle with formatted datasets for training, development and testing on the notMNIST dataset.\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile \n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reload the data we genereted in the pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset (200000, 28, 28) (200000,)\n",
      "validation set  (10000, 28, 28) (10000,)\n",
      "test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file='notMNIST.pickle'\n",
    "\n",
    "\n",
    "with open(pickle_file , 'rb') as f:\n",
    "    save=pickle.load(f)\n",
    "    train_dataset=save ['train_dataset']\n",
    "    train_labels=save ['train_labels']\n",
    "    valid_dataset=save ['valid_dataset']\n",
    "    valid_labels=save ['valid_labels']\n",
    "    test_dataset=save ['test_dataset']\n",
    "    test_labels=save ['test_labels']\n",
    "    del save #to help compiler to free up space \n",
    "    print('train_dataset', train_dataset.shape , train_labels.shape)\n",
    "    print('validation set ', valid_dataset.shape, valid_labels.shape)\n",
    "    print('test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  reformat into a shape that is more adapted to the models we are going to train\n",
    "DATA AS FLAT MATRIX\n",
    "LAB ELS AS FLOAT 1-HOT ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now firstly train a multinomial logistic regression (softmax regression)using simple gradient descent\n",
    "how tensorflow gonna work in here :\n",
    "firstly give all the inputs variable and the formula of our desired computition then these created as nodes over a computation graph \n",
    "with graph.as_default():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with gradient descent training we have use small datasets as it takes longer  time\n",
    "#So we should subset the train data for faster training \n",
    "\n",
    "train_subset=10000\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Input data.\n",
    "    #Load the training ,validation and data into constants that are attached to the graph\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset=tf.constant(valid_dataset)\n",
    "    tf_test_dataset= tf.constant(test_dataset)\n",
    "    \n",
    "    #variables\n",
    "    #these are the parameters that we are going to trainng .the weight matrix in inisialized using random values \n",
    "    #with normal distribution\n",
    "    #the biases get initialized to zero\n",
    "    \n",
    "    weights=tf.Variable(\n",
    "     tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    #print('weights', weights)\n",
    "    biases =tf.Variable(tf.zeros([num_labels]))\n",
    "    #print ('biases', biases)\n",
    "    \n",
    "    \n",
    "    #training computation\n",
    "    #multiply the inputs with the weight matrix , and add biases then compute softmax and cross-entropy \n",
    "    #then take the average of this cross-entropy across all training example : that is our loss\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights)+ biases #scores\n",
    "    loss=  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))#cross-entropy\n",
    "    \n",
    "    #optimizer\n",
    "    #now using gradient descent for minimizing the loss after cross entropy\n",
    "    \n",
    "    optimizer= tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    #predictions for the training , validation and test data\n",
    "    #these are not part of training , but merely ,here so that we can report\n",
    "    #accuracy figure as we train.\n",
    "    \n",
    "    train_prediction=tf.nn.softmax(logits)\n",
    "    valid_prediction=tf.nn.softmax(tf.matmul(tf_valid_dataset,weights) + biases)\n",
    "    test_prediction= tf.nn.softmax(tf.matmul(tf_test_dataset,weights)+ biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run the computation graph iteratly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "loss at step 0: 12.443406\n",
      "training accuracy: 16.8%\n",
      "validation accuracy: 20.1%\n",
      "loss at step 100: 2.323062\n",
      "training accuracy: 71.3%\n",
      "validation accuracy: 70.3%\n",
      "loss at step 200: 1.906028\n",
      "training accuracy: 74.3%\n",
      "validation accuracy: 72.8%\n",
      "loss at step 300: 1.673731\n",
      "training accuracy: 75.5%\n",
      "validation accuracy: 73.7%\n",
      "loss at step 400: 1.510024\n",
      "training accuracy: 76.5%\n",
      "validation accuracy: 74.4%\n",
      "loss at step 500: 1.385775\n",
      "training accuracy: 77.0%\n",
      "validation accuracy: 74.8%\n",
      "loss at step 600: 1.286983\n",
      "training accuracy: 77.6%\n",
      "validation accuracy: 75.2%\n",
      "loss at step 700: 1.205801\n",
      "training accuracy: 78.1%\n",
      "validation accuracy: 75.3%\n",
      "loss at step 800: 1.137442\n",
      "training accuracy: 78.5%\n",
      "validation accuracy: 75.5%\n",
      "test accuracy: 83.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps= 801\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return(100.0 * np.sum(np.argmax(predictions, 1)== np.argmax(labels, 1))/ predictions.shape[0])\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session :\n",
    "    # this is a one-time operation which ensures the parameters initialized as we described in the graph: random\n",
    "    #weights for matrix and zeros for the bias \n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        #run the computations ,we tell . run() that we want to run the optimizer,\n",
    "        #and get the loss value and the training predictions returnes as numpy arrays\n",
    "        _, l, predictions = session.run([optimizer , loss , train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('loss at step %d: %f' % (step, l))\n",
    "            print('training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, : ]))\n",
    "            #Calling.evaL() on valid_prediction is basically like calling run (), but just to get that one np array \n",
    "            #and it recomputes all its graph dependencies\n",
    "            print('validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "         \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we are going to use Stochastic Gradient decent (SDG)\n",
    "This is much faster.\n",
    "The graph will be similar but this time we will not use a constant node to store the training data.\n",
    "we will create a placeholder node which will be fed actual data at every call of session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    #Input data. For the training data, we use a placeholder that will be fed \n",
    "    #at run time with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #Variables\n",
    "    weights= tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
